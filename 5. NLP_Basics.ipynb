{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svogR-QMCHar",
        "outputId": "6c338d8d-c6c5-4671-a56f-968f87dff15d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# importing libraries\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "text = \"Artificial Intelligence (AI) is the technology that enables machines to mimic human thinking and decision-making. It learns from data, identifies patterns, and improves performance over time. AI helps automate tasks, enhance accuracy, and solve complex problems across many industries.\""
      ],
      "metadata": {
        "id": "VcJE-M5pCdzu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# word tokenization\n",
        "words = word_tokenize(text)\n",
        "print(\"Word Tokenization:\", words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRQ5PNaQEsxz",
        "outputId": "ad2461f6-e7a9-4f53-cf6e-b06c3f6c658e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Tokenization: ['Artificial', 'Intelligence', '(', 'AI', ')', 'is', 'the', 'technology', 'that', 'enables', 'machines', 'to', 'mimic', 'human', 'thinking', 'and', 'decision-making', '.', 'It', 'learns', 'from', 'data', ',', 'identifies', 'patterns', ',', 'and', 'improves', 'performance', 'over', 'time', '.', 'AI', 'helps', 'automate', 'tasks', ',', 'enhance', 'accuracy', ',', 'and', 'solve', 'complex', 'problems', 'across', 'many', 'industries', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = sent_tokenize(text)\n",
        "print(\"Sentence Tokenization:\", sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9BoFtA0GAd_",
        "outputId": "8e11b023-12eb-40d3-a7b8-0e11e7afb608"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence Tokenization: ['Artificial Intelligence (AI) is the technology that enables machines to mimic human thinking and decision-making.', 'It learns from data, identifies patterns, and improves performance over time.', 'AI helps automate tasks, enhance accuracy, and solve complex problems across many industries.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of Words\", len(words))\n",
        "print(\"Number of Sentences\", len(sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b24LNG_GIbW",
        "outputId": "d56bfd8a-0b9c-44ac-863b-03c8921e1efc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Words 48\n",
            "Number of Sentences 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in sent_tokenize(text):\n",
        "  for word in word_tokenize(sentence):\n",
        "    print(\"word_tokenization\",word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOtvUo3EGe3G",
        "outputId": "ee711605-2939-43be-c502-a634d02d1273"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word_tokenization Artificial\n",
            "word_tokenization Intelligence\n",
            "word_tokenization (\n",
            "word_tokenization AI\n",
            "word_tokenization )\n",
            "word_tokenization is\n",
            "word_tokenization the\n",
            "word_tokenization technology\n",
            "word_tokenization that\n",
            "word_tokenization enables\n",
            "word_tokenization machines\n",
            "word_tokenization to\n",
            "word_tokenization mimic\n",
            "word_tokenization human\n",
            "word_tokenization thinking\n",
            "word_tokenization and\n",
            "word_tokenization decision-making\n",
            "word_tokenization .\n",
            "word_tokenization It\n",
            "word_tokenization learns\n",
            "word_tokenization from\n",
            "word_tokenization data\n",
            "word_tokenization ,\n",
            "word_tokenization identifies\n",
            "word_tokenization patterns\n",
            "word_tokenization ,\n",
            "word_tokenization and\n",
            "word_tokenization improves\n",
            "word_tokenization performance\n",
            "word_tokenization over\n",
            "word_tokenization time\n",
            "word_tokenization .\n",
            "word_tokenization AI\n",
            "word_tokenization helps\n",
            "word_tokenization automate\n",
            "word_tokenization tasks\n",
            "word_tokenization ,\n",
            "word_tokenization enhance\n",
            "word_tokenization accuracy\n",
            "word_tokenization ,\n",
            "word_tokenization and\n",
            "word_tokenization solve\n",
            "word_tokenization complex\n",
            "word_tokenization problems\n",
            "word_tokenization across\n",
            "word_tokenization many\n",
            "word_tokenization industries\n",
            "word_tokenization .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming"
      ],
      "metadata": {
        "id": "T0nvRFrEIT4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "wXnegFUVIYqe"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "\n",
        "words = [\"running\", \"jump\", \"easily\", \"fairly\", \"studies\"]\n",
        "\n",
        "stems = [stemmer.stem(word) for word in words]\n",
        "\n",
        "print(\"stems:\", stems)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwPjg4VdIie-",
        "outputId": "b229d272-e3e7-4982-ec38-9ffb4628aa33"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stems: ['run', 'jump', 'easili', 'fairli', 'studi']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lemmatization"
      ],
      "metadata": {
        "id": "sRFkNUHxTNWo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk import pos_tag, word_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJXF01chIpQH",
        "outputId": "d4aec082-4b75-4fe3-b35a-c480d4b576db"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "words = [\"running\", \"jump\", \"easily\", \"fairly\", \"studies\"]"
      ],
      "metadata": {
        "id": "6sM5h9tWUUAg"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_wordnet_pos(word):\n",
        "  tag = pos_tag([word])[0] [1] [0].upper()\n",
        "  tag_dict = {\"J\": wordnet.ADJ,\n",
        "              \"N\": wordnet.NOUN,\n",
        "              \"V\": wordnet.VERB,\n",
        "              \"R\": wordnet.ADV}\n",
        "  return tag_dict.get(tag, wordnet.NOUN)"
      ],
      "metadata": {
        "id": "NpVslikbUO7A"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmas = [Lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in words]\n",
        "print(\"lemmas:\", lemmas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69SrnwAWVOpA",
        "outputId": "64c0cef2-29bb-4cda-e0f1-77c8e72d7001"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lemmas: ['run', 'jump', 'easily', 'fairly', 'study']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remove Stop Words"
      ],
      "metadata": {
        "id": "xmaFrxFSV0lR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-aGWmoTV4Rn",
        "outputId": "cca9f372-8b09-47bc-a48c-01a09afa256d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"This is an example sentence that showing off stop word filteration.\"\n",
        "words = word_tokenize(text)\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "filtered_sentence = [w for w in words if not w.lower() in stop_words]\n",
        "\n",
        "print(\"Orginal sentence\", words)\n",
        "print(\"Filtered sentence\", filtered_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6Fgzh2sWD3g",
        "outputId": "eea87f62-c79d-47b7-8b44-f87f92e5536f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Orginal sentence ['This', 'is', 'an', 'example', 'sentence', 'that', 'showing', 'off', 'stop', 'word', 'filteration', '.']\n",
            "Filtered sentence ['example', 'sentence', 'showing', 'stop', 'word', 'filteration', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NER (Named Entity Recognition)"
      ],
      "metadata": {
        "id": "c6QczlXjtj1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "spacy.cli.download(\"en_core_web_sm\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Ns7ut5ftro4",
        "outputId": "23793fe3-ddc9-4879-acb3-61273bffe03d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"Apple is looking at buying U.K. startup for $1 billion\"\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "\n",
        "print(\"Entities:\", entities)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vE6PTyPUV8KA",
        "outputId": "31e190bd-e4ab-48c3-9d11-8cdb6fd6096e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entities: [('Apple', 'ORG'), ('U.K.', 'GPE'), ('$1 billion', 'MONEY')]\n"
          ]
        }
      ]
    }
  ]
}